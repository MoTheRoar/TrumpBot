{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e4f17757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1ebcbf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1ee86c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "15cc05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trump_tweets.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7b9be0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 583429\n",
      "Total chars: 54\n",
      "Number of sequences: 194443\n"
     ]
    }
   ],
   "source": [
    "text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\n",
    "print(\"Corpus length:\", len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print(\"Total chars:\", len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 100\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i : i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print(\"Number of sequences:\", len(sentences))\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.float32)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.float32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c9f0739a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel.load_weights(\\'test_generation_2.h5\\') \\nmodel.build(tensorflow.TensorShape([1, None]))\\n#print(model.summary())\\ndef generate_text(model, start_string):\\n    print(\\'Generating with seed: \"\\' + start_string + \\'\"\\')\\n  \\n    num_generate = 1000\\n    input_eval = [char_indices[s] for s in start_string]\\n    input_eval = tensorflow.expand_dims(input_eval, 0)\\n    input_eval = tensorflow.reshape(input_eval, [1, 1, 11])\\n    print(input_eval)\\n    text_generated = []\\n    temperature = 1.0\\n    model.reset_states()\\n    for i in range(num_generate):\\n        predictions = model(input_eval)\\n        predictions = tensorflow.squeeze(predictions, 0)\\n        predictions = predictions / temperature\\n        predicted_id = tensorflow.random.categorical(predictions,      num_samples=1)[-1,0].numpy()\\n        input_eval = tensorflow.expand_dims([predicted_id], 0)\\n        text_generated.append(indices_char[predicted_id])\\n    return (start_string + \\'\\'.join(text_generated))\\nprint(generate_text(model, start_string=\"joy of gods\"))\\n'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import tensorflow.python.ops.numpy_ops.np_config\n",
    "#np_config.enable_numpy_behavior()\n",
    "\n",
    "'''\n",
    "model.load_weights('test_generation_2.h5') \n",
    "model.build(tensorflow.TensorShape([1, None]))\n",
    "#print(model.summary())\n",
    "def generate_text(model, start_string):\n",
    "    print('Generating with seed: \"' + start_string + '\"')\n",
    "  \n",
    "    num_generate = 1000\n",
    "    input_eval = [char_indices[s] for s in start_string]\n",
    "    input_eval = tensorflow.expand_dims(input_eval, 0)\n",
    "    input_eval = tensorflow.reshape(input_eval, [1, 1, 11])\n",
    "    print(input_eval)\n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tensorflow.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tensorflow.random.categorical(predictions,      num_samples=1)[-1,0].numpy()\n",
    "        input_eval = tensorflow.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(indices_char[predicted_id])\n",
    "    return (start_string + ''.join(text_generated))\n",
    "print(generate_text(model, start_string=\"joy of gods\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813638d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e798c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = tensorflow.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e788f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(len(chars)*4, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('selu'))\n",
    "\n",
    "    model.add(Dense(len(chars)*4))\n",
    "    model.add(Activation('selu'))\n",
    "\n",
    "    model.add(Dense(len(chars)*4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('selu'))\n",
    "\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "    optimizer = tensorflow.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3c1e25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:   3%|██▏                                                              | 4/120 [00:00<00:03, 29.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IRS is a scam\n",
      "he IRS is a scame\n",
      "e IRS is a scame \n",
      " IRS is a scame o\n",
      "IRS is a scame or\n",
      "RS is a scame or \n",
      "S is a scame or C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating text:   7%|████▎                                                            | 8/120 [00:00<00:03, 31.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is a scame or Ch\n",
      "is a scame or Chi\n",
      "s a scame or Chin\n",
      " a scame or China\n",
      "a scame or China \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  13%|████████▌                                                       | 16/120 [00:00<00:03, 32.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " scame or China V\n",
      "scame or China Vi\n",
      "came or China Vir\n",
      "ame or China Viru\n",
      "me or China Virus\n",
      "e or China Virus \n",
      " or China Virus i\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  20%|████████████▊                                                   | 24/120 [00:00<00:03, 31.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or China Virus in\n",
      "r China Virus in \n",
      " China Virus in A\n",
      "China Virus in Am\n",
      "hina Virus in Ame\n",
      "ina Virus in Amer\n",
      "na Virus in Ameri\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating text:  23%|██████████████▉                                                 | 28/120 [00:00<00:02, 30.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Virus in Americ\n",
      " Virus in America\n",
      "Virus in America \n",
      "irus in America a\n",
      "rus in America an\n",
      "us in America and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  29%|██████████████████▋                                             | 35/120 [00:01<00:03, 27.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s in America and \n",
      " in America and I\n",
      "in America and I \n",
      "n America and I w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  34%|█████████████████████▊                                          | 41/120 [00:01<00:03, 25.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " America and I wi\n",
      "America and I wil\n",
      "merica and I will\n",
      "erica and I will \n",
      "rica and I will b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating text:  37%|███████████████████████▍                                        | 44/120 [00:01<00:03, 25.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ica and I will be\n",
      "ca and I will be \n",
      "a and I will be g\n",
      " and I will be gi\n",
      "and I will be giv\n",
      "nd I will be givi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  42%|██████████████████████████▋                                     | 50/120 [00:01<00:02, 24.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d I will be givin\n",
      " I will be giving\n",
      "I will be giving \n",
      " will be giving a\n",
      "will be giving a \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  47%|█████████████████████████████▊                                  | 56/120 [00:02<00:02, 24.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ill be giving a s\n",
      "ll be giving a st\n",
      "l be giving a sto\n",
      " be giving a stor\n",
      "be giving a story\n",
      "e giving a story \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating text:  49%|███████████████████████████████▍                                | 59/120 [00:02<00:02, 22.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " giving a story T\n",
      "giving a story To\n",
      "iving a story To \n",
      "ving a story To B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  54%|██████████████████████████████████▋                             | 65/120 [00:02<00:02, 22.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ing a story To Bu\n",
      "ng a story To But\n",
      "g a story To But \n",
      " a story To But I\n",
      "a story To But I \n",
      " story To But I w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  59%|█████████████████████████████████████▊                          | 71/120 [00:02<00:02, 24.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story To But I wi\n",
      "tory To But I wil\n",
      "ory To But I will\n",
      "ry To But I will \n",
      "y To But I will b\n",
      " To But I will be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  64%|█████████████████████████████████████████                       | 77/120 [00:02<00:01, 24.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To But I will be \n",
      "o But I will be a\n",
      " But I will be a \n",
      "But I will be a t\n",
      "ut I will be a te\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  69%|████████████████████████████████████████████▎                   | 83/120 [00:03<00:01, 25.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t I will be a ter\n",
      " I will be a terr\n",
      "I will be a terri\n",
      " will be a terrif\n",
      "will be a terrifi\n",
      "ill be a terrific\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  74%|███████████████████████████████████████████████▍                | 89/120 [00:03<00:01, 25.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll be a terrific \n",
      "l be a terrific T\n",
      " be a terrific Th\n",
      "be a terrific The\n",
      "e a terrific They\n",
      " a terrific They \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  79%|██████████████████████████████████████████████████▋             | 95/120 [00:03<00:00, 25.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a terrific They w\n",
      " terrific They wi\n",
      "terrific They wil\n",
      "errific They will\n",
      "rrific They will \n",
      "rific They will b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  84%|█████████████████████████████████████████████████████          | 101/120 [00:03<00:00, 26.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ific They will be\n",
      "fic They will be \n",
      "ic They will be a\n",
      "c They will be a \n",
      " They will be a t\n",
      "They will be a te\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  89%|████████████████████████████████████████████████████████▏      | 107/120 [00:04<00:00, 26.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey will be a ter\n",
      "ey will be a terr\n",
      "y will be a terri\n",
      " will be a terrif\n",
      "will be a terrifi\n",
      "ill be a terrific\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  94%|███████████████████████████████████████████████████████████▎   | 113/120 [00:04<00:00, 23.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll be a terrific \n",
      "l be a terrific T\n",
      " be a terrific Th\n",
      "be a terrific The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating text:  97%|████████████████████████████████████████████████████████████▉  | 116/120 [00:04<00:00, 24.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e a terrific They\n",
      " a terrific They \n",
      "a terrific They w\n",
      " terrific They wi\n",
      "terrific They wil\n",
      "errific They will\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text: 100%|███████████████████████████████████████████████████████████████| 120/120 [00:04<00:00, 25.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rrific They will \n",
      "Seed: rific They will b\n",
      "Generated text:\n",
      "e or China Virus in America and I will be giving a story To But I will be a terrific They will be a terrific They will b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('test_generation_2.h5') \n",
    "\n",
    "seed = 'A Victory for Biden, and a Bet on America’s Future'\n",
    "n_chars = 120\n",
    "sequence_length = 120\n",
    "# generate 400 characters\n",
    "generated = \"\"\n",
    "for i in tqdm.tqdm(range(n_chars), \"Generating text\"):\n",
    "    # make the input sequence\n",
    "    X = np.zeros((1, sequence_length, vocab_size))\n",
    "    for t, char in enumerate(seed):\n",
    "        X[0, (sequence_length - len(seed)) + t, char_indices[char]] = 1\n",
    "    # predict the next character\n",
    "    predicted = model.predict(X, verbose=0)[0]\n",
    "    # converting the vector to an integer\n",
    "    next_index = np.argmax(predicted)\n",
    "    # converting the integer to a character\n",
    "    next_char = indices_char[next_index]\n",
    "    # add the character to results\n",
    "    generated += next_char\n",
    "    # shift seed and the predicted character\n",
    "    seed = seed[1:] + next_char\n",
    "\n",
    "print(\"Seed:\", seed)\n",
    "print(\"Generated text:\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "19a9c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "                \n",
    "checkpoint_path = r'training\\check.ckpt'\n",
    "checkpoint_directory = os.path.dirname(checkpoint_path)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='loss', patience=2, verbose=0),\n",
    "    ModelCheckpoint(checkpoint_directory, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8fd0c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('test_generation_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf070c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8c01a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "045d47ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.makedirs('Generated Text')\n",
    "except:\n",
    "    print('Already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.tetaefa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c2252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef65cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.fit(x, y, batch_size=batch_size, epochs=1, callbacks=callbacks)\n",
    "    print()\n",
    "    print(\"Generating text after epoch: %d\" % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 0.7]:\n",
    "        print(\"...Diversity:\", diversity)\n",
    "\n",
    "        generated = \"\"\n",
    "        sentence = text[start_index : start_index + maxlen]\n",
    "        print('...Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "        for i in range(120):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.0\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "            sentence = sentence[1:] + next_char\n",
    "            generated += next_char\n",
    "\n",
    "        with open(os.path.join('Generated Text', f'gen_text_batch 64 batch epoch 40'), 'a', encoding='utf-8') as f:\n",
    "            f.write(f'\"...Diversity:\" {diversity} \\n\"...Generating with seed: \" {sentence}\\n ...Generated: {generated}\\n\\n')\n",
    "\n",
    "\n",
    "        print(\"...Generated: \", generated)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e43997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfnogpu",
   "language": "python",
   "name": "tfnogpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
