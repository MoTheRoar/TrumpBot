{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f17757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ebcbf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee86c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15cc05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trump_tweets.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b9be0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 583429\n",
      "Total chars: 54\n",
      "Number of sequences: 194443\n"
     ]
    }
   ],
   "source": [
    "text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\n",
    "print(\"Corpus length:\", len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print(\"Total chars:\", len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 100\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i : i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print(\"Number of sequences:\", len(sentences))\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.float32)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.float32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f0739a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel.load_weights(\\'test_generation_2.h5\\') \\nmodel.build(tensorflow.TensorShape([1, None]))\\n#print(model.summary())\\ndef generate_text(model, start_string):\\n    print(\\'Generating with seed: \"\\' + start_string + \\'\"\\')\\n  \\n    num_generate = 1000\\n    input_eval = [char_indices[s] for s in start_string]\\n    input_eval = tensorflow.expand_dims(input_eval, 0)\\n    input_eval = tensorflow.reshape(input_eval, [1, 1, 11])\\n    print(input_eval)\\n    text_generated = []\\n    temperature = 1.0\\n    model.reset_states()\\n    for i in range(num_generate):\\n        predictions = model(input_eval)\\n        predictions = tensorflow.squeeze(predictions, 0)\\n        predictions = predictions / temperature\\n        predicted_id = tensorflow.random.categorical(predictions,      num_samples=1)[-1,0].numpy()\\n        input_eval = tensorflow.expand_dims([predicted_id], 0)\\n        text_generated.append(indices_char[predicted_id])\\n    return (start_string + \\'\\'.join(text_generated))\\nprint(generate_text(model, start_string=\"joy of gods\"))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import tensorflow.python.ops.numpy_ops.np_config\n",
    "#np_config.enable_numpy_behavior()\n",
    "\n",
    "'''\n",
    "model.load_weights('test_generation_2.h5') \n",
    "model.build(tensorflow.TensorShape([1, None]))\n",
    "#print(model.summary())\n",
    "def generate_text(model, start_string):\n",
    "    print('Generating with seed: \"' + start_string + '\"')\n",
    "  \n",
    "    num_generate = 1000\n",
    "    input_eval = [char_indices[s] for s in start_string]\n",
    "    input_eval = tensorflow.expand_dims(input_eval, 0)\n",
    "    input_eval = tensorflow.reshape(input_eval, [1, 1, 11])\n",
    "    print(input_eval)\n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tensorflow.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tensorflow.random.categorical(predictions,      num_samples=1)[-1,0].numpy()\n",
    "        input_eval = tensorflow.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(indices_char[predicted_id])\n",
    "    return (start_string + ''.join(text_generated))\n",
    "print(generate_text(model, start_string=\"joy of gods\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "813638d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, \"'\": 1, 'A': 2, 'B': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7, 'G': 8, 'H': 9, 'I': 10, 'J': 11, 'K': 12, 'L': 13, 'M': 14, 'N': 15, 'O': 16, 'P': 17, 'Q': 18, 'R': 19, 'S': 20, 'T': 21, 'U': 22, 'V': 23, 'W': 24, 'X': 25, 'Y': 26, 'Z': 27, 'a': 28, 'b': 29, 'c': 30, 'd': 31, 'e': 32, 'f': 33, 'g': 34, 'h': 35, 'i': 36, 'j': 37, 'k': 38, 'l': 39, 'm': 40, 'n': 41, 'o': 42, 'p': 43, 'q': 44, 'r': 45, 's': 46, 't': 47, 'u': 48, 'v': 49, 'w': 50, 'x': 51, 'y': 52, 'z': 53}\n"
     ]
    }
   ],
   "source": [
    "print(char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e798c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = Sequential()\\nmodel.add(LSTM(256, return_sequences=True))\\nmodel.add(Dropout(0.2))\\nmodel.add(LSTM(256))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(len(chars), activation=\\'softmax\\'))\\n\\noptimizer = tensorflow.keras.optimizers.RMSprop(learning_rate=0.01)\\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = tensorflow.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e788f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(len(chars)*4, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('selu'))\n",
    "\n",
    "    model.add(Dense(len(chars)*4))\n",
    "    model.add(Activation('selu'))\n",
    "\n",
    "    model.add(Dense(len(chars)*4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('selu'))\n",
    "\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "    optimizer = tensorflow.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c1e25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text: 100%|███████████████████████████████████████████████████████████████| 120/120 [00:03<00:00, 37.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: A Victory for Biden and a Bet on Americas Future\n",
      "Generated text:\n",
      " I am all over to resulf the USA is Strong on Crime Couch Schumer and State are not for the White House and the Washingt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('test_generation_2.h5') \n",
    "\n",
    "seed = 'A Victory for Biden and a Bet on Americas Future'\n",
    "seed_new = seed\n",
    "n_chars = 120\n",
    "sequence_length = 120\n",
    "# generate 400 characters\n",
    "generated = \"\"\n",
    "for i in tqdm.tqdm(range(n_chars), \"Generating text\"):\n",
    "    # make the input sequence\n",
    "    X = np.zeros((1, sequence_length, len(chars)))\n",
    "    for t, char in enumerate(seed):\n",
    "        X[0, (sequence_length - len(seed)) + t, char_indices[char]] = 1\n",
    "    # predict the next character\n",
    "    predicted = model.predict(X, verbose=0)[0]\n",
    "    # converting the vector to an integer\n",
    "    next_index = np.argmax(predicted)\n",
    "    # converting the integer to a character\n",
    "    next_char = indices_char[next_index]\n",
    "    # add the character to results\n",
    "    generated += next_char\n",
    "    # shift seed and the predicted character\n",
    "    seed = seed[1:] + next_char\n",
    "\n",
    "print(\"Seed:\", seed_new)\n",
    "print(\"Generated text:\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19a9c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "                \n",
    "checkpoint_path = r'training\\check.ckpt'\n",
    "checkpoint_directory = os.path.dirname(checkpoint_path)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='loss', patience=2, verbose=0),\n",
    "    ModelCheckpoint(checkpoint_directory, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('test_generation_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf070c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c01a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.makedirs('Generated Text')\n",
    "except:\n",
    "    print('Already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.tetaefa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c2252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef65cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.fit(x, y, batch_size=batch_size, epochs=1, callbacks=callbacks)\n",
    "    print()\n",
    "    print(\"Generating text after epoch: %d\" % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 0.7]:\n",
    "        print(\"...Diversity:\", diversity)\n",
    "\n",
    "        generated = \"\"\n",
    "        sentence = text[start_index : start_index + maxlen]\n",
    "        print('...Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "        for i in range(120):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.0\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "            sentence = sentence[1:] + next_char\n",
    "            generated += next_char\n",
    "\n",
    "        with open(os.path.join('Generated Text', f'gen_text_batch 64 batch epoch 40'), 'a', encoding='utf-8') as f:\n",
    "            f.write(f'\"...Diversity:\" {diversity} \\n\"...Generating with seed: \" {sentence}\\n ...Generated: {generated}\\n\\n')\n",
    "\n",
    "\n",
    "        print(\"...Generated: \", generated)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e43997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfnogpu",
   "language": "python",
   "name": "tfnogpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
